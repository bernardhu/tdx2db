package cmd

import (
	"context"
	"database/sql"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"runtime"
	"sort"
	"sync"
	"sync/atomic"

	_ "github.com/duckdb/duckdb-go/v2"
	"github.com/jing2uo/tdx2db/database"
	"github.com/jing2uo/tdx2db/model"
	"github.com/jing2uo/tdx2db/tdx"
	"github.com/jing2uo/tdx2db/utils"
)

var GP_FILE_URL = "https://data.tdx.com.cn/tdxgp/"
var GP_ALL_URL = "https://data.tdx.com.cn/vipdoc/"

func Gp2(dbPath, gpFileDir string, download bool) error {
	if dbPath == "" {
		return fmt.Errorf("database path cannot be empty")
	}
	dbConfig := model.DBConfig{Path: dbPath}
	db, err := database.Connect(dbConfig)
	if err != nil {
		return fmt.Errorf("failed to connect to database: %w", err)
	}
	defer db.Close()

	fmt.Printf("ğŸ“¦ å¼€å§‹å¤„ç†è‚¡ç¥¨ç›®å½•: %s\n", gpFileDir)
	err = utils.CheckDirectory(gpFileDir)
	if err != nil {
		return err
	}

	targetPath := filepath.Join(gpFileDir, "gpszsh.txt")
	existingHashes, err := loadHashes(targetPath)
	filterHashes(existingHashes)
	if err != nil {
		return fmt.Errorf("failed to read existing gpcw cache: %w", err)
	}

	url := "https://data.tdx.com.cn/tdxgp/gpszsh.txt"
	status, err := utils.DownloadFile(url, targetPath)
	if err != nil {
		return fmt.Errorf("failed to download gpcw.txt: %w", err)
	}

	switch status {
	case 200:
		fmt.Print("âœ… å·²ä¸‹è½½ gpszsh.txt\n")
	case 404:
		fmt.Printf("ğŸŸ¡ gpszsh.txt æ— æ³•è®¿é—®\n")
		return nil
	default:
		fmt.Printf("âš ï¸ gpszsh.txt è¿”å›çŠ¶æ€ç : %d\n", status)
		return nil
	}

	latestHashes, err := loadHashes(targetPath)
	if err != nil {
		return fmt.Errorf("failed to read latest gpcw.txt: %w", err)
	}

	filterHashes(latestHashes)
	updatedFiles, olds, news := diffHashes(existingHashes, latestHashes)
	if len(updatedFiles) == 0 {
		fmt.Println("â„¹ï¸ æ²¡æœ‰æ–°çš„è‚¡ç¥¨æ–‡ä»¶éœ€è¦æ›´æ–°")
		return nil
	}

	sort.Strings(updatedFiles)
	fmt.Printf("ğŸŒŸ å‘ç° %d ä¸ªè‚¡ç¥¨æ–‡ä»¶å˜æ›´ oldhash:%v newhash:%v\n", len(updatedFiles), olds, news)

	updatedSet := make(map[string]struct{}, len(updatedFiles))
	for _, f := range updatedFiles {
		updatedSet[f] = struct{}{}
	}
	/*
		if len(updatedSet) > 2000 { //å…¨éƒ¨ä¸‹è½½ç®—äº†
			fmt.Printf("â•will try download all\n")
			zipPath := filepath.Join(gpFileDir, "tdxgp.zip")
			if err := downloadFile(zipPath, "tdxgp.zip", GP_ALL_URL, true); err != nil {
				return err
			}
			cmd := exec.Command("rm", "-f", gpFileDir+"/*.dat")
			cmd.Stdout = os.Stdout
			cmd.Stderr = os.Stderr

			if err := cmd.Run(); err != nil {
				fmt.Printf("âš ï¸ åˆ é™¤æ—§æ–‡ä»¶å¤±è´¥\n")
				return nil
			}

			if err := utils.UnzipFile(zipPath, gpFileDir); err != nil {
				return fmt.Errorf("failed to unzip file %s: %w", targetPath, err)
			}
			return nil
		}
	*/

	allFiles := make([]string, 0, len(latestHashes))
	for f := range latestHashes {
		allFiles = append(allFiles, f)
	}
	sort.Strings(allFiles)

	var stockFiles []string
	var blkFiles []string
	var mktFiles []string
	for _, f := range allFiles {
		_, _, res := parseFileName(f)
		typ := res
		if res == "ashare" {
			typ = "stock"
		}
		switch typ {
		case "stock":
			stockFiles = append(stockFiles, f)
		case "tdx":
			blkFiles = append(blkFiles, f)
		case "mkt":
			mktFiles = append(mktFiles, f)
		}
	}

	if err := rebuildGpBaseFromFiles(db, gpFileDir, stockFiles, updatedSet, download); err != nil {
		return err
	}

	// å…¶ä»–å°æ–‡ä»¶ï¼ˆæ¿å—/å¸‚åœºï¼‰ä»æŒ‰åŸé€»è¾‘å•çº¿ç¨‹å¯¼å…¥
	for _, v := range append(blkFiles, mktFiles...) {
		targetPath := filepath.Join(gpFileDir, v)
		if err := downloadFile(targetPath, v, GP_FILE_URL, download); err != nil {
			return err
		}

		mkt, code, res := parseFileName(v)
		typ := res
		if res == "ashare" {
			typ = "stock"
		}

		recs, err := tdx.ParseGpDAT(targetPath, mkt, code)
		if err != nil {
			return fmt.Errorf("failed to parse file %s: %w", targetPath, err)
		}

		if typ == "tdx" { //block
			if err := database.ImportBlkdata(db, recs); err != nil {
				return fmt.Errorf("failed to import blk file %w", err)
			}
		} else if typ == "mkt" { //mkt
			if err := database.ImportMktdata(db, recs); err != nil {
				return fmt.Errorf("failed to import mkt file %w", err)
			}
		}
	}

	fmt.Printf("å¼€å§‹åˆ›å»ºè§†å›¾\n")
	err = database.CreateGpViews(db)
	if err != nil {
		fmt.Printf("åˆ›å»ºè§†å›¾å¤±è´¥ err: %v\n", err)
	}
	err = database.CreateMktViews(db)
	if err != nil {
		fmt.Printf("åˆ›å»ºè§†å›¾å¤±è´¥ err: %v\n", err)
	}
	err = database.CreateBlkViews(db)
	if err != nil {
		fmt.Printf("åˆ›å»ºè§†å›¾å¤±è´¥ err: %v\n", err)
	}
	return nil
}

func rebuildGpBaseFromFiles(db *sql.DB, gpFileDir string, files []string, updatedSet map[string]struct{}, download bool) error {
	if len(files) == 0 {
		fmt.Println("â„¹ï¸ æœªå‘ç° stock ç±»å‹æ–‡ä»¶ï¼Œè·³è¿‡ GP é‡å»º")
		return nil
	}

	workerCount := runtime.GOMAXPROCS(0)
	if workerCount < 1 {
		workerCount = 1
	}
	if workerCount > len(files) {
		workerCount = len(files)
	}

	fmt.Printf("ğŸš€ GP å…¨é‡é‡å»º: files=%d workers=%d\n", len(files), workerCount)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	jobs := make(chan string, workerCount*2)
	batches := make(chan database.GpWideBatch, workerCount*2)

	writerErrCh := make(chan error, 1)
	go func() {
		err := database.RebuildGpBase(ctx, db, batches)
		if err != nil {
			cancel()
		}
		writerErrCh <- err
	}()

	var workerErr error
	var workerErrOnce sync.Once
	setWorkerErr := func(err error) {
		workerErrOnce.Do(func() {
			workerErr = err
			cancel()
		})
	}

	var processed atomic.Int64
	total := int64(len(files))

	//process file
	var wg sync.WaitGroup
	wg.Add(workerCount)
	for i := 0; i < workerCount; i++ {
		go func() {
			defer wg.Done()
			for {
				select {
				case <-ctx.Done():
					return
				case v, ok := <-jobs:
					if !ok {
						return
					}

					targetPath := filepath.Join(gpFileDir, v)
					if err := downloadFile(targetPath, v, GP_FILE_URL, download); err != nil {
						setWorkerErr(err)
						return
					}

					mkt, code, _ := parseFileName(v)
					recs, err := tdx.ParseGpDAT(targetPath, mkt, code)
					if err != nil {
						setWorkerErr(fmt.Errorf("failed to parse file %s: %w", targetPath, err))
						return
					}

					batch, err := database.AggregateGpRecords(recs)
					if err != nil {
						setWorkerErr(fmt.Errorf("failed to aggregate file %s: %w", targetPath, err))
						return
					}

					batches <- batch

					n := processed.Add(1)
					if n%200 == 0 || n == total {
						fmt.Printf("ğŸ“ˆ GP è¿›åº¦: %d/%d\n", n, total)
					}
				}
			}
		}()
	}

	//dispatch file
	go func() {
		defer close(jobs)
		for _, f := range files {
			select {
			case jobs <- f:
			case <-ctx.Done():
				return
			}
		}
	}()

	wg.Wait()
	close(batches)

	writerErr := <-writerErrCh
	if workerErr != nil {
		return workerErr
	}
	if writerErr != nil && writerErr != context.Canceled {
		return writerErr
	}
	return nil
}

// https://data.tdx.com.cn/vipdoc/tdxgp.zip
func downloadFile(targetPath, fileName, urlbase string, download bool) error {
	if !download {
		return nil
	}

	url := urlbase + fileName
	cmd := exec.Command("wget", "-O", targetPath, url)
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr

	if err := cmd.Run(); err != nil {
		return fmt.Errorf("âš ï¸ wget ä¸‹è½½ %s å¤±è´¥: %w", url, err)
	}
	return nil
}
